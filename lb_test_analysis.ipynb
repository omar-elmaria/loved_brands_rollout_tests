{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import the Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\My Drive\\APAC\\Autopricing\\Switchback Testing\\switchback_test_dag\\venv_sb\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "from scipy.stats import wilcoxon\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Instantiate a BQ client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client(project=\"logistics-data-staging-flat\")\n",
    "bqstorage_client = bigquery_storage.BigQueryReadClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Import the SQL queries and run them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_queries = [i for i in os.listdir(path=os.getcwd() + \"/sql_queries\") if i.endswith(\".sql\")]\n",
    "\n",
    "for i in sql_queries:\n",
    "    with open(file=os.getcwd() + f\"/sql_queries/{i}\", mode=\"r\") as sql:\n",
    "        client.query(query=sql.read()).result()\n",
    "        sql.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Pull the order data from the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 4043531/4043531 [1:07:18<00:00, 1001.23rows/s] \n"
     ]
    }
   ],
   "source": [
    "df = client\\\n",
    "    .query(query=\"\"\"SELECT * FROM `dh-logistics-product-ops.pricing.ab_test_individual_orders_cleaned_lb_rollout_tests`\"\"\")\\\n",
    "    .result()\\\n",
    "    .to_dataframe(bqstorage_client=bqstorage_client, progress_bar_type=\"tqdm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4: Create a dataset of the business KPIs analyzed on the treatment scope level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of df\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of some columns to an appropriate type\n",
    "df_copy[[\"test_id\", \"dps_travel_time_fee_local\", \"gfv_local\", \"gmv_local\", \"commission_local\"]] = df_copy[[\"test_id\", \"dps_travel_time_fee_local\", \"gfv_local\", \"gmv_local\", \"commission_local\"]]\\\n",
    "    .apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "# Find the first and last date of an experiment and eliminate those two dates\n",
    "df_dates = df_copy.groupby([\"region\", \"entity_id\", \"test_id\", \"test_name\"], as_index=False)[\"created_date_utc\"].agg({\"min_date\": \"min\", \"max_date\": \"max\"})\n",
    "\n",
    "# Join df_dates to df\n",
    "df_copy = pd.merge(left=df_copy, right=df_dates[[\"entity_id\", \"test_id\", \"min_date\", \"max_date\"]], how=\"left\", on=[\"entity_id\", \"test_id\"])\n",
    "\n",
    "# Filter out the orders that occurred on the first or last day of the experiment\n",
    "df_copy = df_copy[(df_copy[\"created_date_utc\"] > df_copy[\"min_date\"]) & (df_copy[\"created_date_utc\"] < df_copy[\"max_date\"])]\n",
    "\n",
    "# Create a KPI list\n",
    "kpi_list = [\"actual_df_paid_by_customer\", \"dps_travel_time_fee_local\", \"gfv_local\", \"gmv_local\", \"commission_local\", \"revenue_local\", \"delivery_costs_local\", \"gross_profit_local\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that aggregates the metrics and computes the p-values on any granularity (experiment, treatment/non-treatment scope, and target group)\n",
    "def pval_func(granularity): # granularity can be \"experiment\", \"treatment\", \"non_treatment\", \"target_group\"\n",
    "    if granularity == \"experiment\":\n",
    "        grouping_vars_for_agg = [\"region\", \"entity_id\", \"test_id\", \"test_name\", \"variant\", \"created_date_utc\"]\n",
    "    elif granularity == \"treatment\" or granularity == \"non_treatment\":\n",
    "        grouping_vars_for_agg = [\"region\", \"entity_id\", \"test_id\", \"test_name\", \"is_in_treatment\", \"variant\", \"created_date_utc\"]\n",
    "    elif granularity == \"target_group\":\n",
    "        grouping_vars_for_agg = [\"region\", \"entity_id\", \"test_id\", \"test_name\", \"target_group_bi\", \"variant\", \"created_date_utc\"]\n",
    "\n",
    "    # Calculate the total KPIs per entity_id, test_id, variant, and date\n",
    "    df_copy_agg = df_copy.groupby(grouping_vars_for_agg)[kpi_list].sum()\n",
    "\n",
    "    # Add the order count to df_copy_agg\n",
    "    df_copy_agg[\"order_count\"] = df_copy.groupby(grouping_vars_for_agg)[\"order_id\"].nunique()\n",
    "\n",
    "    # Reset the index of df_copy_agg\n",
    "    df_copy_agg.reset_index(inplace=True)\n",
    "\n",
    "    return df_copy_agg\n",
    "\n",
    "# Create multiple data frames containing the right data cut for each granularity\n",
    "df_exp = pval_func(granularity=\"experiment\")\n",
    "df_treatment = pval_func(granularity=\"treatment\")\n",
    "df_treatment = df_treatment[df_treatment[\"is_in_treatment\"]==True]\n",
    "df_non_treatment = pval_func(granularity=\"non_treatment\")\n",
    "df_non_treatment = df_non_treatment[df_non_treatment[\"is_in_treatment\"]==False]\n",
    "df_target_group = pval_func(granularity=\"target_group\")\n",
    "\n",
    "# Create a data frame containing the distinct combinations of test_name and variant. This is the data frame that will receive the p-values from the wilcoxon function\n",
    "test_var_list = [\"test_name\", \"variant\"]\n",
    "df_pval = df_exp[df_exp[\"variant\"] != \"Control\"][test_var_list].drop_duplicates().reset_index(drop=True) # We could use any \"df\" here, not only df_exp\n",
    "df_pval[\"key\"] = 1\n",
    "\n",
    "# Create a data frame containing all granularities\n",
    "granularity_list = [\"experiment\", \"treatment\", \"non_treatment\"]\n",
    "tg_list = df_copy[\"target_group_bi\"].dropna().unique().tolist()\n",
    "granularity_list.extend(tg_list)\n",
    "df_granularity = pd.DataFrame(granularity_list, columns=[\"granularity\"])\n",
    "df_granularity[\"key\"] = 1\n",
    "\n",
    "# Create a data frame containing the names of the KPIs\n",
    "kpi_list_with_orders = [\"order_count\"]\n",
    "kpi_list_with_orders.extend(kpi_list)\n",
    "df_kpi_list_with_order_count = pd.DataFrame(kpi_list_with_orders, columns=[\"kpi\"])\n",
    "df_kpi_list_with_order_count[\"key\"] = 1\n",
    "\n",
    "# Perfrom a cross join to get the distinct combinations of test_name, variant, and KPI\n",
    "df_pval = pd.merge(left=df_pval, right=df_granularity, how=\"left\", on=\"key\")\n",
    "df_pval = pd.merge(left=df_pval, right=df_kpi_list_with_order_count, how=\"left\", on=\"key\").drop(\"key\", 1)\n",
    "\n",
    "# Add the p-value column. The None values will be replaced with actual values by the Wilcoxon function\n",
    "df_pval[\"pval\"] = None\n",
    "\n",
    "# Iterate over each test_name, variant, and KPI and calculate the p-value between variation(x) and control\n",
    "for i in range(len(df_pval)):\n",
    "    test_iter = df_pval.loc[i, \"test_name\"]\n",
    "    variant_iter = df_pval.loc[i, \"variant\"]\n",
    "    granularity_iter = df_pval.loc[i, \"granularity\"]\n",
    "    kpi_iter = df_pval.loc[i, \"kpi\"]\n",
    "\n",
    "    # Choosing the right data cuts based on the granularity\n",
    "    if granularity_iter == \"experiment\":\n",
    "        df_calc = df_exp.copy()\n",
    "    elif granularity_iter == \"treatment\":\n",
    "        df_calc = df_treatment.copy()\n",
    "    elif granularity_iter == \"non_treatment\":\n",
    "        df_calc = df_non_treatment.copy()\n",
    "    elif granularity_iter == \"Target Group 1\":\n",
    "        df_calc = df_target_group[df_target_group[\"target_group_bi\"] == \"Target Group 1\"].copy()\n",
    "    elif granularity_iter == \"Target Group 2\":\n",
    "        df_calc = df_target_group[df_target_group[\"target_group_bi\"] == \"Target Group 2\"].copy()\n",
    "    elif granularity_iter == \"Target Group 3\":\n",
    "        df_calc = df_target_group[df_target_group[\"target_group_bi\"] == \"Target Group 3\"].copy()\n",
    "    elif granularity_iter == \"Target Group 4\":\n",
    "        df_calc = df_target_group[df_target_group[\"target_group_bi\"] == \"Target Group 4\"].copy()\n",
    "\n",
    "    try:\n",
    "        df_pval.loc[i, \"pval\"] = wilcoxon(\n",
    "            x=df_calc[(df_calc[\"variant\"] == variant_iter) & (df_calc[\"test_name\"] == test_iter)][kpi_iter],\n",
    "            y=df_calc[(df_calc[\"variant\"] == \"Control\") & (df_calc[\"test_name\"] == test_iter)][kpi_iter],\n",
    "            zero_method=\"wilcox\",\n",
    "            correction=False,\n",
    "            alternative=\"two-sided\",\n",
    "            method=\"auto\",\n",
    "            nan_policy=\"omit\"\n",
    "        ).pvalue\n",
    "    except ValueError as err: # Sometimes, samples x and y are not equal in length due to missing data. If that's the case, set the p-value to None\n",
    "        df_pval.loc[i, \"pval\"] = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.5: Upload df_pval to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the job_config to overwrite the data in the table\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "\n",
    "# Upload the p-values data frame to BQ\n",
    "job = client.load_table_from_dataframe(\n",
    "    dataframe=df_pval,\n",
    "    destination=\"dh-logistics-product-ops.pricing.p_vals_lb_rollout_tests\",\n",
    "    job_config=job_config\n",
    ")\n",
    "\n",
    "# Wait for the load job to complete\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv_sb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14e010e4cd1c1ecfc2a757c09121a44deab645fe879881bec23ed2eed3f5394d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
