{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import the Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\My Drive\\APAC\\Autopricing\\Switchback Testing\\switchback_test_dag\\venv_sb\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "from scipy.stats import wilcoxon\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Instantiate a BQ client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client(project=\"logistics-data-staging-flat\")\n",
    "bqstorage_client = bigquery_storage.BigQueryReadClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Import the SQL queries and run them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_queries = [i for i in os.listdir(path=os.getcwd() + \"/sql_queries\") if i.endswith(\".sql\")]\n",
    "\n",
    "for i in sql_queries:\n",
    "    with open(file=os.getcwd() + f\"/sql_queries/{i}\", mode=\"r\") as sql:\n",
    "        client.query(query=sql.read()).result()\n",
    "        sql.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Pull the order data from the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 204016/204016 [01:02<00:00, 3245.29rows/s]\n"
     ]
    }
   ],
   "source": [
    "df = client\\\n",
    "    .query(query=\"\"\"SELECT * FROM `dh-logistics-product-ops.pricing.ab_test_individual_orders_cleaned_lb_rollout_tests`\"\"\")\\\n",
    "    .result()\\\n",
    "    .to_dataframe(bqstorage_client=bqstorage_client, progress_bar_type=\"tqdm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4: Create a dataset of the business KPIs analyzed on the treatment scope level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of df\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of some columns to an appropriate type\n",
    "df_copy[[\"test_id\", \"dps_travel_time_fee_local\", \"gfv_local\", \"gmv_local\", \"commission_local\"]] = df_copy[[\"test_id\", \"dps_travel_time_fee_local\", \"gfv_local\", \"gmv_local\", \"commission_local\"]]\\\n",
    "    .apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "# Find the first and last date of an experiment and eliminate those two dates\n",
    "df_dates = df_copy.groupby([\"region\", \"entity_id\", \"test_id\", \"test_name\"], as_index=False)[\"created_date_utc\"].agg({\"min_date\": \"min\", \"max_date\": \"max\"})\n",
    "\n",
    "# Join df_dates to df\n",
    "df_copy = pd.merge(left=df_copy, right=df_dates[[\"entity_id\", \"test_id\", \"min_date\", \"max_date\"]], how=\"left\", on=[\"entity_id\", \"test_id\"])\n",
    "\n",
    "# Filter out the orders that occurred on the first or last day of the experiment\n",
    "df_copy = df_copy[(df_copy[\"created_date_utc\"] > df_copy[\"min_date\"]) & (df_copy[\"created_date_utc\"] < df_copy[\"max_date\"])]\n",
    "\n",
    "# Create a KPI list\n",
    "kpi_list = [\"actual_df_paid_by_customer\", \"dps_travel_time_fee_local\", \"gfv_local\", \"gmv_local\", \"commission_local\", \"revenue_local\", \"delivery_costs_local\", \"gross_profit_local\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that aggregates the metrics and computes the p-values on any granularity (experiment, treatment/non-treatment scope, and target group)\n",
    "def pval_func(granularity): # granularity can be \"experiment\", \"treatment\", \"non_treatment\", \"target_group\"\n",
    "    if granularity == \"Experiment\":\n",
    "        grouping_vars_for_agg = [\"region\", \"entity_id\", \"test_id\", \"test_name\", \"variant\", \"created_date_utc\"]\n",
    "    elif granularity == \"Treatment Scope\" or granularity == \"Non-Treatment Scope\":\n",
    "        grouping_vars_for_agg = [\"region\", \"entity_id\", \"test_id\", \"test_name\", \"is_in_treatment\", \"variant\", \"created_date_utc\"]\n",
    "    elif granularity == \"Target Group\":\n",
    "        grouping_vars_for_agg = [\"region\", \"entity_id\", \"test_id\", \"test_name\", \"target_group_bi\", \"variant\", \"created_date_utc\"]\n",
    "\n",
    "    # Calculate the total KPIs per entity_id, test_id, variant, and date\n",
    "    df_copy_agg = df_copy.groupby(grouping_vars_for_agg)[kpi_list].sum()\n",
    "\n",
    "    # Add the order count to df_copy_agg\n",
    "    df_copy_agg[\"order_count\"] = df_copy.groupby(grouping_vars_for_agg)[\"order_id\"].nunique()\n",
    "\n",
    "    # Reset the index of df_copy_agg\n",
    "    df_copy_agg.reset_index(inplace=True)\n",
    "\n",
    "    return df_copy_agg\n",
    "\n",
    "# Create multiple data frames containing the right data cut for each granularity\n",
    "df_exp = pval_func(granularity=\"Experiment\")\n",
    "df_treatment = pval_func(granularity=\"Treatment Scope\")\n",
    "df_treatment = df_treatment[df_treatment[\"is_in_treatment\"]==True]\n",
    "df_non_treatment = pval_func(granularity=\"Non-Treatment Scope\")\n",
    "df_non_treatment = df_non_treatment[df_non_treatment[\"is_in_treatment\"]==False]\n",
    "df_target_group = pval_func(granularity=\"Target Group\")\n",
    "\n",
    "# Create a data frame containing the distinct combinations of test_name and variant. This is the data frame that will receive the p-values from the wilcoxon function\n",
    "grouping_var_list = [\"region\", \"entity_id\", \"test_name\", \"variant\"]\n",
    "df_pval = df_exp[df_exp[\"variant\"] != \"Control\"][grouping_var_list].drop_duplicates().reset_index(drop=True) # We could use any \"df\" here, not only df_exp\n",
    "df_pval[\"key\"] = 1\n",
    "\n",
    "# Create a data frame containing all granularities\n",
    "granularity_list = [\"Experiment\", \"Treatment Scope\", \"Non-Treatment Scope\"]\n",
    "tg_list = df_copy[\"target_group_bi\"].dropna().unique().tolist()\n",
    "granularity_list.extend(tg_list)\n",
    "df_granularity = pd.DataFrame(granularity_list, columns=[\"granularity\"])\n",
    "df_granularity[\"key\"] = 1\n",
    "\n",
    "# Create a data frame containing the names of the KPIs\n",
    "kpi_list_with_orders = [\"order_count\"]\n",
    "kpi_list_with_orders.extend(kpi_list)\n",
    "df_kpi_list_with_order_count = pd.DataFrame(kpi_list_with_orders, columns=[\"kpi\"])\n",
    "df_kpi_list_with_order_count[\"key\"] = 1\n",
    "\n",
    "# Perfrom a cross join to get the distinct combinations of test_name, variant, and KPI\n",
    "df_pval = pd.merge(left=df_pval, right=df_granularity, how=\"left\", on=\"key\")\n",
    "df_pval = pd.merge(left=df_pval, right=df_kpi_list_with_order_count, how=\"left\", on=\"key\").drop(\"key\", 1)\n",
    "\n",
    "# Add the p-value column. The None values will be replaced with actual values by the Wilcoxon function\n",
    "df_pval[\"pval\"] = None\n",
    "\n",
    "# Iterate over each test_name, variant, and KPI and calculate the p-value between variation(x) and control\n",
    "for i in range(len(df_pval)):\n",
    "    test_iter = df_pval.loc[i, \"test_name\"]\n",
    "    variant_iter = df_pval.loc[i, \"variant\"]\n",
    "    granularity_iter = df_pval.loc[i, \"granularity\"]\n",
    "    kpi_iter = df_pval.loc[i, \"kpi\"]\n",
    "\n",
    "    # Choosing the right data cuts based on the granularity\n",
    "    if granularity_iter == \"Experiment\":\n",
    "        df_calc = df_exp.copy()\n",
    "    elif granularity_iter == \"Treatment Scope\":\n",
    "        df_calc = df_treatment.copy()\n",
    "    elif granularity_iter == \"Non-Treatment Scope\":\n",
    "        df_calc = df_non_treatment.copy()\n",
    "    elif granularity_iter == \"Target Group 1\":\n",
    "        df_calc = df_target_group[df_target_group[\"target_group_bi\"] == \"Target Group 1\"].copy()\n",
    "    elif granularity_iter == \"Target Group 2\":\n",
    "        df_calc = df_target_group[df_target_group[\"target_group_bi\"] == \"Target Group 2\"].copy()\n",
    "    elif granularity_iter == \"Target Group 3\":\n",
    "        df_calc = df_target_group[df_target_group[\"target_group_bi\"] == \"Target Group 3\"].copy()\n",
    "    elif granularity_iter == \"Target Group 4\":\n",
    "        df_calc = df_target_group[df_target_group[\"target_group_bi\"] == \"Target Group 4\"].copy()\n",
    "\n",
    "    try:\n",
    "        df_pval.loc[i, \"pval\"] = wilcoxon(\n",
    "            x=df_calc[(df_calc[\"variant\"] == variant_iter) & (df_calc[\"test_name\"] == test_iter)][kpi_iter],\n",
    "            y=df_calc[(df_calc[\"variant\"] == \"Control\") & (df_calc[\"test_name\"] == test_iter)][kpi_iter],\n",
    "            zero_method=\"wilcox\",\n",
    "            correction=False,\n",
    "            alternative=\"two-sided\",\n",
    "            method=\"auto\",\n",
    "            nan_policy=\"omit\"\n",
    "        ).pvalue\n",
    "    except ValueError as err: # Sometimes, samples x and y are not equal in length due to missing data. If that's the case, set the p-value to None\n",
    "        df_pval.loc[i, \"pval\"] = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.5: Upload df_pval to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the format of df_pval\n",
    "df_pval_formatted = pd.pivot_table(data=df_pval, values=\"pval\", index=[\"test_name\", \"variant\", \"granularity\"], columns=\"kpi\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>kpi</th>\n",
       "      <th>test_name</th>\n",
       "      <th>variant</th>\n",
       "      <th>granularity</th>\n",
       "      <th>actual_df_paid_by_customer</th>\n",
       "      <th>commission_local</th>\n",
       "      <th>delivery_costs_local</th>\n",
       "      <th>dps_travel_time_fee_local</th>\n",
       "      <th>gfv_local</th>\n",
       "      <th>gmv_local</th>\n",
       "      <th>gross_profit_local</th>\n",
       "      <th>order_count</th>\n",
       "      <th>revenue_local</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PH_20221209_LovedBrandsPampanga_Updated</td>\n",
       "      <td>Variation1</td>\n",
       "      <td>Experiment</td>\n",
       "      <td>0.706253</td>\n",
       "      <td>0.946101</td>\n",
       "      <td>0.663581</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.388380</td>\n",
       "      <td>0.202311</td>\n",
       "      <td>0.794220</td>\n",
       "      <td>0.388380</td>\n",
       "      <td>0.992295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PH_20221209_LovedBrandsPampanga_Updated</td>\n",
       "      <td>Variation2</td>\n",
       "      <td>Experiment</td>\n",
       "      <td>0.594814</td>\n",
       "      <td>0.327150</td>\n",
       "      <td>0.317572</td>\n",
       "      <td>0.005442</td>\n",
       "      <td>0.869576</td>\n",
       "      <td>0.839256</td>\n",
       "      <td>0.281065</td>\n",
       "      <td>0.735217</td>\n",
       "      <td>0.263893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PH_20221209_LovedBrandsPampanga_Updated</td>\n",
       "      <td>Variation3</td>\n",
       "      <td>Experiment</td>\n",
       "      <td>0.094161</td>\n",
       "      <td>0.027436</td>\n",
       "      <td>0.479773</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.467763</td>\n",
       "      <td>0.720686</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.504277</td>\n",
       "      <td>0.014852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "kpi                                test_name     variant granularity  \\\n",
       "0    PH_20221209_LovedBrandsPampanga_Updated  Variation1  Experiment   \n",
       "5    PH_20221209_LovedBrandsPampanga_Updated  Variation2  Experiment   \n",
       "10   PH_20221209_LovedBrandsPampanga_Updated  Variation3  Experiment   \n",
       "\n",
       "kpi  actual_df_paid_by_customer  commission_local  delivery_costs_local  \\\n",
       "0                      0.706253          0.946101              0.663581   \n",
       "5                      0.594814          0.327150              0.317572   \n",
       "10                     0.094161          0.027436              0.479773   \n",
       "\n",
       "kpi  dps_travel_time_fee_local  gfv_local  gmv_local  gross_profit_local  \\\n",
       "0                     0.003350   0.388380   0.202311            0.794220   \n",
       "5                     0.005442   0.869576   0.839256            0.281065   \n",
       "10                    0.000036   0.467763   0.720686            0.001853   \n",
       "\n",
       "kpi  order_count  revenue_local  \n",
       "0       0.388380       0.992295  \n",
       "5       0.735217       0.263893  \n",
       "10      0.504277       0.014852  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pval_formatted[(df_pval_formatted[\"test_name\"].str.contains(\"Pampanga\")) & (df_pval_formatted[\"granularity\"].str.contains(\"Experiment\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=logistics-data-staging-flat, location=US, id=c5e0083b-0563-40c9-85f5-caa8fed5d724>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the job_config to overwrite the data in the table\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "\n",
    "# Upload the p-values data frame to BQ\n",
    "job = client.load_table_from_dataframe(\n",
    "    dataframe=df_pval,\n",
    "    destination=\"dh-logistics-product-ops.pricing.p_vals_lb_rollout_tests\",\n",
    "    job_config=job_config\n",
    ")\n",
    "\n",
    "# Wait for the load job to complete\n",
    "job.result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv_sb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14e010e4cd1c1ecfc2a757c09121a44deab645fe879881bec23ed2eed3f5394d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
